
services:
  # Kafka service for real-time event streaming
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-kraft
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-kraft:9093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-kraft:29092,CONTROLLER://kafka-kraft:9093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT_HOST://localhost:9092,PLAINTEXT://kafka-kraft:29092,CONTROLLER://kafka-kraft:9093'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      CLUSTER_ID: '8hmf5K_tQwaupe4sKU6AwQ'
    volumes:
      - kafka_data:/var/lib/kafka/data

  # --- NEW: A dedicated container for our Spark application ---
  spark-app:
    build: . # Tells Docker to build the image from the Dockerfile in this directory
    container_name: spark-app
    volumes:
      # Mount the data directory so Spark can read/write files from/to the host
      - ./data:/app/data
      # Mount source and config for live code changes without rebuilding
      - ./src:/app/src
      - ./config:/app/config
    # This command keeps the container running in the background
    # so we can execute our Spark job inside it.
    command: tail -f /dev/null

# Named volumes to persist data
volumes:
  kafka_data:
    driver: local